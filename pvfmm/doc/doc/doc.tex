\documentclass[11pt]{article}
\usepackage{amsmath,amssymb}
\usepackage{subfigure}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{url}
\usepackage{fullpage}
%\usepackage{program}
\usepackage{epsfig}
\graphicspath{{figures/}}
\usepackage[usenames,dvipsnames]{color}

\renewcommand{\algorithmiccomment}[1]{\textcolor{OliveGreen}{//#1}}
\newcommand\LONGCOMMENT[1]{\textcolor{OliveGreen}{/* #1 */}}

%\newcommand\PARFOR[1]{\\\textbf{parfor} {#1} \textbf{do}\\ \begin{minipage}[b]{0.1\linewidth} { } \end{minipage} \begin{minipage}[b]{\linewidth} }
%\newcommand\ENDPARFOR{\end{minipage}\\\textbf{end parfor}}
\newcommand\PARFOR[1]{\hspace{20 mm}\\\textbf{par}\algorithmicfor\ {#1}\ \algorithmicdo\\\begin{ALC@for}}
\newcommand\ENDPARFOR{\end{ALC@for}\algorithmicend\ \textbf{par}\algorithmicfor}

\title{KIFMM based Poisson Solver}
\author{Dhairya Malhotra}
%\date{April 12, 2011}

\begin{document}
\maketitle


\section{Problem Statement}
To build a scalable free space Poisson solver using Chebyshev Trees and Kernel 
Independent Fast Multipole Method in three dimensions using MPI and OpenMP for 
parallelism.

\section{Outline}
The algorithm consists of two major components, the octree construction and the 
Fast Multipole evaluation.


\subsection{Octree Construction}
We construct a piecewise polynomial representation of the input from point 
samples. A vector of point coordinates and input function values at those 
points is taken as the input. The coordinates and the values are sorted 
according to their Morton id using a distributed sample sort and a OpenMP 
version of merge sort (\ref{merge_sort}). Next, an octree is constructed from 
these points by specifying the maximum number of points per box. The sorted 
array of points is partitioned among the processes and each process 
independently constructs a linear octree for its local set of points using the 
algorithm p2oLocal (\ref{p2o_local}). Finally, each of the local octrees is 
merged, i.e. the first and the last octant of each partial octree is merged
with the last and the first octant on the previous and the next process 
respectively. All of this is done using the tree construction package DENDRO
\cite{Sampath_dendro:parallel}.


A chebyshev approximation of the input function is now generated for each 
octant of the octree. This is embaressingly parallel. We, determine the 
coefficients $c_{i,j,k}$ by solving the following least squares problem.

\begin{equation}
\underset{c_{i,j,k}}{\operatorname{argmin}}\sum_{p\in{B}}{\Big(f_p-\sum_{0 \le i,j,k<n}{c_{i,j,k}T_{i}(x_p)T_{j}(y_p)T_{k}(z_p)}\Big)^2}
\end{equation}

where, $B$ is the set of set of points in an octant, each point $p$ has 
associated with it the input value ($f_p$) at that point and the coordinates of 
the point $(x_p,y_p,z_p)$. $T_i(x)$ is the Chebyshev polynomial of degree $i$ in 
$x$. Given the coefficients $c_{i,j,k}$, the Chebyshev approximation $f_{T}(x,y,z)$ 
is given by:

\begin{equation}
f_{T}(x,y,z) = \sum_{0 \le i,j,k<n}{c_{i,j,k}T_{i}(x)T_{j}(y)T_{k}(z)}
\end{equation}


\subsection{Fast Multipole Method Evaluation}
We evaluate the Green's function solution of the Poisson's Equation.

\begin{equation}
\Delta{u} = f \notag
\end{equation}
\begin{equation}
u(r') = \int\limits_{r \in D} {G(r,r')f(r)} \notag
\end{equation}

where, $G(r,r')=\frac{1}{|r-r'|}$ , $D$ is the cubic domain of an octant, $f$ 
is the source distribution defined over the domin $D$ and $u$ is the required 
free space solution to the Poisson's equation.

We use Kernel Independent FMM to compute the above integral.
The near interactions are computed by direct integration, for which we use Duffy
transformation to evaluate the singular integrals (described in section 
\ref{sec:singular_integ}). We precompute the integrals over the Chebyshev 
polynomials and finally evaluate the integral for the Chebyshev approximation 
as linear operation in the coefficients of the approximation.

The multipole expansion and U-list interactions for leaf octants is computed 
using direct integration. The target coordinates are the Chebyshev-Gauss points,
and we finally compute a Chebyshev approximation of the potential from the 
values at the target points. The rest of the algorithm for KIFMM is the same as 
described in \cite{Ying03akernel-independent}.

All translations (source-to-multipole, multipole-to-multipole, 
multipole-to-local, etc.) are represented by matrix multiplications and we use 
BLAS to perform these computations efficiently. For the V-list evaluation, we 
use the FFT acceleration described in \cite{Ying03akernel-independent}.

The reduce-broadcast of multipole expansions following the upward pass is 
achieved using the algorithms described in (\ref{multipole_reduce},
\ref{multipole_broadcast}).

The overall algorithm for FMM evaluation is described below (\ref{fmm_eval}).


\begin{algorithm}[H]
\caption{$FMM\_Eval$}
\label{fmm_eval}
\begin{algorithmic}
  \STATE $T$ : Input Chebyshev octree.
  \STATE
  \STATE 1) Compute Multipole Expansion for all leaf nodes.
  \STATE 2) Evaluate U2U translation in post-order.
  \STATE
  \STATE 3) Reduce Broadcast of multipole expansions.
  \STATE
  \STATE 4) Evaluate U,V,W and X interactions for nodes.
  \STATE 5) Evaluate D2D translation in pre-order.
  \STATE
  \STATE 6) Chebyshev approx from target values for all leaf nodes.
\end{algorithmic}
\end{algorithm}

\paragraph{Time Complexity:}
$T(n,p) = O(n/p + max\_depth) + T_{comm}$\\
Where, $n$ is the number of octants in the octree and $p$ is the number of 
processors and $T_{comm}$ is the communication cost.

The U2U and D2D translations, shared memory parallelim is implemented in a 
level by level manner. Translations are evaluated for all nodes at a particular 
level in parallel. All other computations are embarrassingly parallel for all 
nodes. The time complexity of the communication step is discused below along 
with their pseudocodes (\ref{multipole_reduce},\ref{multipole_broadcast}).




\section{Pseudocodes and Complexity Analysis}

The pseudocodes for the major components are described below

\subsection{Parallel Merge (Multi-threaded)}


\begin{algorithm}[H]
\caption{$ParMerge (Multi-threaded)$}
\label{par_merge}
\begin{algorithmic}
  \STATE $A[0 : n1-1]$ : First input vector.
  \STATE $B[0 : n2-1]$ : Second input vector.
  \STATE $C[0 : n-1 ]$ : Output vector ($n=n1+n2$).
  \STATE $p$ : Number of OMP threads.

  \STATE $m=~10$ (a small constant integer.)
  \STATE $S0[0:2*p*m]$ : Array
  \STATE $S1[0:p]$     : Array

  \STATE \LONGCOMMENT {Each thread samples the arrays $A$ and $B$ at m points and determines the point at which 
                   this will split the final array $C$. Then out of these samples each thread selects the 
                   optimal splitters i.e. which divide the final array $C$ at points $i*n/p$ for $i=1$ to $p-1$.}
  \PARFOR {$i=0$ to $p-1$}
    \FOR {$j=0$ to $m-1$}
      \STATE $S0[i*m+j]     = ${ The index at which the element $A[(i*m+j)*(n1/p/m)]$ will split $C$. }
      \STATE $S0[i*m+j+p*m] = ${ The index at which the element $B[(i*m+j)*(n2/p/m)]$ will split $C$. }
    \ENDFOR
  \ENDPARFOR
  \STATE {}
  \PARFOR {$i=1$ to $p-1$}
    \STATE $S1[i] = ${ Find in $S0$ the index closest to $i*n/p$. }
  \ENDPARFOR
  \STATE $S1[0]=0$
  \STATE $S1[p]=n$
  \STATE {}
  \PARFOR {$i=0$ to $p-1$}
    \STATE $C[ S1[i] , ... , S1[i+1]-1 ] = $ \{$seq\_merge()$ to get this partial result\}
  \ENDPARFOR

\end{algorithmic}
\end{algorithm}

\paragraph{Time Complexity:}
$T(n,p) = O(m*log(n) + log(p) + n/p)$\\
Where, n is the size of the array to be sorted and p is the number of 
processors.


\subsection{Merge Sort (Multi-threaded)}

\begin{algorithm}[H]
\caption{$MergeSort (Multi-threaded)$}
\label{merge_sort}
\begin{algorithmic}
  \STATE $A[0:n-1]$: Input vector 
  \STATE $p$       : Number of OMP threads
  \STATE \COMMENT {For simplicity, assume n to be multiple of p.}
  \STATE \COMMENT {And assume p be a power of 2.}
  \STATE {}
  \PARFOR {$i = 0$ to $p-1$}
    \STATE $seq\_sort(A[ {i*n/p} : {(i+1)*n/p} ])$
  \ENDPARFOR
  \STATE \COMMENT {Merge pairs of sorted sub-sections.}
  \STATE $i=2*n/p$
  \WHILE {$i \leq n$}
    \FOR {$j=0$ to $n/i-1$}
      \STATE $par\_merge( A[j*i, ... ,j*i+i/2] , A[j*i+i/2, ... ,(j+1)*i] )$
    \ENDFOR
  \STATE $i=i*2$
  \ENDWHILE
\end{algorithmic}
\end{algorithm}

\paragraph{Time Complexity:}
$T(n,p) = O((n/p)*log(n/p)) + O((n/p)*log(p))$ \\
(Assuming parallel merge to be O(n/p)) \\
Where, n is the size of the array to be sorted and p is the number of 
processors.


\subsection{Points to Octree (Local)}

\begin{algorithm}[H]
\caption{$p2oLocal (Sequential)$}
\label{p2o_local}
\begin{algorithmic}
  \STATE $N$ : Total Number of points
  \STATE $m$ : Maximum points per node.
  \STATE $A[1,...,N]$ : Sorted array of Morton id.
  \STATE {}
  \STATE $i = 0$
  \STATE $curr\_node = root\_node$ \COMMENT {This is an iterator which performs a preorder traversal.}
  \WHILE {$i < N$}
    \STATE $nxt\_mid = curr\_node.next\_mid()$ \COMMENT {Morton id of the next node.}
    \IF {$nxt\_mid > A[i+N]$}
      \STATE $curr\_node.subdivide()$
      \STATE $curr\_node = curr\_node.child(0)$ \COMMENT {First child (least Morton id).}
    \ELSE
      \STATE $k = bin\_search( A[i,...,i+N], nxt\_mid )$
      \STATE $curr\_node.assign\_points( A[i, i+1, ... , k-1] )$
      \STATE $curr\_node = curr\_node.preorder\_next()$ \COMMENT {Next node in preorder traversal.}
      \STATE $i = k$
    \ENDIF
  \ENDWHILE
\end{algorithmic}
\end{algorithm}

\paragraph{Time Complexity:}
$T(N) = O( \frac{N}{m} \log m )$ \\
Where, N is the total number of points and m is the maximum number of points 
per node of the tree.

\paragraph{OpenMP Parallelization:}
Shared memory parallelization can be achieved similar to distributed memory 
parallelization. The array of sorted points is partitioned equally between each 
thread. The octree is now constructed for each part independently by each 
thread. And the final octree is obtained by mergin the first and last octant of 
each partial octree with the previous and next partial octree respectively.


\subsection{Multipole Reduce}
Following the upward FMM pass, we need to add up the multipole expansions of 
the nodes which span regions controlled by more than one processor. The 
communication between processes required for the reduction of multipole 
expansion is mapped to a hypercube network topology. Each process identifies 
the list of tree nodes it shares with other processes. These will be the 
ancestors of either the first or the last node in the morton sorted list of 
leaf nodes.

Now, in each step, a process exchanges shared nodes with another process that 
it is directly connected to in the hypercube topology, moving in the order of 
least significant dimension to the most significant dimension of the hypercube.
In each step, shared tree nodes among pairs of adjacent regions are merged 
forming a bigger regions such that the only nodes which still need to be 
updated are shared across the new bigger regions and the nodes contained 
entirely within a region now have the correct multipole expansions. At any 
stage in this process, each processor maintains a list of nodes which its 
region (the region to which the process belongs) shares with adjacent regions, 
i.e. a maximum of $2 * d$ nodes, where d is the maximum depth of the tree. In 
each communication step, the shared nodes are exchanged and then each process 
independently adds up the multipole expansions of the nodes shared between the 
two regions and builds the list of nodes shared by the new region with adjacent 
regions.

\begin{algorithm}[H]
\caption{$MULTIPOLE\_REDUCE$}
\label{multipole_reduce}
\begin{algorithmic}
  \STATE $pid$: Rank of this process.
  \STATE $p$ : Number of MPI processes.
  \STATE $T$  : The partial tree of this process.
  \STATE {}
  \STATE \COMMENT {Get references to shared nodes.}
  \STATE $S1 = \{n: n \in $Ancestors$(\min $ LeafNodes$(T))\}$
  \STATE $S2 = \{n: n \in $Ancestors$(\max $ LeafNodes$(T))\}$
  \FOR {$i=0$ to $\log p$}
    \STATE $partner = pid$ XOR $2^i$
    \STATE Send$([S1, S2], partner)$
    \STATE Recv$([R1, R2], partner)$
    \IF {$pid \le partner$}
      \STATE Reduce($S2$, $R1$)
      \STATE $S2 = R2$
    \ELSE
      \STATE Reduce($S1$, $R2$)
      \STATE $S1 = R1$
    \ENDIF
  \ENDFOR
\end{algorithmic}
\end{algorithm}

\paragraph{Time Complexity:}
$T(n)=O(t_s \log p + t_w \log p \log N + \log p \log N)$ \\
Where, $t_s$, $t_w$ are the communication latency and the per-word transfer 
time respectively, $p$ is the number of processes, $N$ is the number of nodes 
in the complete tree.




\subsection{Multipole Broadcast}
After the Multipole Reduce step, each octant has its correct multipole 
expansion. Now, we build the Locally Essential Tree (LET) by sending octants 
from its owner to each of its users. Since we have already completed the 
multipole reduction, we make sure that each node that must be broadcast 
originates from exactly one process and therefore, we avoid the process of 
removing duplicates.

\begin{algorithm}[H]
\caption{$MULTIPOLE\_BROADCAST$}
\label{multipole_broadcast}
\begin{algorithmic}
  \STATE $pid$: Rank of this process.
  \STATE $p$ : Number of MPI processes.
  \STATE $T$  : The partial tree of this process.
  \STATE $mins[]$: Array of minimum Morton id for each process.
  \STATE $P_u(x)$: Set of user processes of node x.
  \STATE {}
  \STATE \COMMENT {Find nodes to be sent.}
  \STATE $S = \{s : s \in T$ and $\left|{P_u(s)}\right|>1$ and $s<mins[pid]\}$
  \FOR {$i=\log p$ to $0$}
    \STATE $partner = pid$ XOR $2^i$
    \STATE $n1 = partner$ AND $(p - 2^i)$
    \STATE $S1 = \{s : s \in S$ and $P_u(s) \cap \{n1,...,n1+2^i-1\} \neq \emptyset\}$
    \STATE Send$(S1,partner)$
    \STATE Recv$(R1,partner)$
    \STATE $n2 = pid$ AND $(p - 2^i)$
    \STATE $S2 = \{s : s \in S$ and $P_u(s) \cap \{n2,...,n1+2^i-1\} \neq \emptyset\}$
    \STATE $S = S2 \cup R1$
  \ENDFOR
  \STATE \textbf{return} $T \cup S$
\end{algorithmic}
\end{algorithm}

\paragraph{Time Complexity:}
$T(n)=O(t_s \log p + t_w k (3 \sqrt{p} - 2))$ \\
This is the same as that described in \cite{I_Lashuk_2009}.




\section{Evaluating Singular Integrals}
\label{sec:singular_integ}
For dirct interactions, we need to evaluate integrals of the following form.

\begin{equation}
U(r')=\iiint\limits_D{G(r,r')\,p(r)\,\mathrm{d}r}
\end{equation}

where, $G(r,r')=\frac{1}{|r-r'|}$ , $D$ is the cubic domain of an octant and 
$p(r)$ is the polynomial approximation of the source density within the octant.
This is a singular integral when $r'$ is on the boundary of $D$ or inside $D$ 
(i.e. $r' \in D$). A simple tensor-product Gauss quadrature rule will converge 
very slowly for values of $r'$ within or close to $D$. To evaluate such 
integrals, we make use of the Duffy transformation followed by a tensor-product 
Gauss quadrature rule.


Consider the following integral over a regular pyramid.

\begin{equation}
U=\int\limits_{0}^{1}\int\limits_{-x}^{x}\int\limits_{-x}^{x}{\frac{p(x,y,z)}{\sqrt{x^2+y^2+z^2}}\,\mathrm{d}z\mathrm{d}y\mathrm{d}x}
\end{equation}

We perform the following change of variables and transform the integration 
domain to a cuboid.

\begin{equation}
y=u\,x \notag
\end{equation}
\begin{equation}
z=v\,x \notag
\end{equation}
\begin{equation}
U=\int\limits_{0}^{1}\int\limits_{-1}^{1}\int\limits_{-1}^{1}{\frac{x \cdot p(x,u\,x,v\,x)}{\sqrt{1^2+u^2+v^2}}\,\mathrm{d}v\mathrm{d}u\mathrm{d}x}
\end{equation}

This transformed equation does not have a singularity and can now be integrated 
using a tensor-product Gauss quadrature rule. Moreover, the integral with 
respect to $x$ can be computed exactly by choosing the order of the rule 
appropriately in the $x$-direction \cite{Mousavi_2010}.


To evaluate the singular integral over a cubic domain (an octant), we partition 
the domain in to six regular pyramidal regions with the apex of the pyramids at 
the singularity. The intersection of each pyramid with the volume of the cube 
can be represented as stacks of square frustums and a smaller pyramid. The 
integral over each of these components is individually evaluated using the 
technique described above using Duffy transformation. 




\section{Scalability Results}
The folowing performance results were obtained on the ION cluster (8 nodes x 2 
x Intel Xeon X5550 quad-core Nehalem processors @ 2.66GHz).

\subsection{Strong Scalability}
The following strong scalability results were obtained by using a constant 
input size of $2^{20}$ points. The octree was constructed with a maximum of 
$2000$ points per octant and using fourth degree Chebyshev polynomials. The 
number of points on equivalent and check surfaces was 488 points.
Up to eight processors, a single compute node was used while increasing the 
number of OpenMP threads. Subsequently, the number of compute nodes was 
increased was increased while keeping the number OpenMP threads fixed to eigth.

In the tree constrution phase, the cost of evaluating the pseudoinverse for 
computing chebyshev approximation dominates the running time. Computing 
chebyshev approximation take about two orders of magnitude more time than all 
the other operations combined. We see a speedup of ~31x using 64 processors 
(48\% efficiency). Theoretically, we should see perfect speedup since 
evaluating chebyshev approximation is embarrassingly parallel.

For the FMM evaluation, the communication costs are negligible compared to the 
computation cost. Therefore, theoretically we should see perfect scalability. 
However, we see a ~29x speedup using 64 processors (45\% efficiency).


\begin{figure}[H]
\centering
%\includegraphics[width=0.75\textwidth]{figures/strong/timing.eps}
\caption{Strong scalability timing results up to 64 processors.}
\end{figure}

\begin{figure}[H]
\centering
%\includegraphics[width=0.75\textwidth]{figures/strong/speedup.eps}
\caption{Strong scalability speed-up results up to 64 processors.}
\end{figure}

\begin{figure}[H]
\centering
%\includegraphics[width=0.75\textwidth]{figures/strong/efficiency.eps}
\caption{Strong scalability efficiency results up to 64 processors.}
\end{figure}


\subsection{Weak Scalability}
The following weak scalability results were obtained by using an input size of 
$2^{18}$ points per processor. The octree was constructed with a maximum of 
$2000$ points per octant and using fourth degree Chebyshev polynomials. The 
number of points on equivalent and check surfaces was 488 points.
Up to eight processors, a single compute node was used while increasing the 
number of OpenMP threads. Subsequently, the number of compute nodes was 
increased was increased while keeping the number OpenMP threads fixed to eigth.

Here, we see performance similar to that of strong scalability. The speedup for
tree construction is 33x with 64 processors (51\% efficiency). For FMM 
evaluation, the speedup is 29x using 64 processors (45\% efficiency). 

\begin{figure}[H]
\centering
%\includegraphics[width=0.75\textwidth]{figures/weak/timing.eps}
\caption{Weak scalability timing results up to 64 processors.}
\end{figure}

\begin{figure}[H]
\centering
%\includegraphics[width=0.75\textwidth]{figures/weak/speedup.eps}
\caption{Weak scalability speed-up results up to 64 processors.}
\end{figure}

\begin{figure}[H]
\centering
%\includegraphics[width=0.75\textwidth]{figures/weak/efficiency.eps}
\caption{Weak scalability efficiency results up to 64 processors.}
\end{figure}


\bibliographystyle{siam}
\bibliography{bibsync.bib}


\end{document}

